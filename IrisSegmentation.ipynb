{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1ZeyASa_LNl8DqZ6Q9CfRUMnTAtxNjgGZ?usp=drive_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "uNmHWQkbJa1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import v2 as T\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_pil_image, to_tensor\n"
      ],
      "metadata": {
        "id": "muvYnbizsyi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "0dojGtqdryc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0247ef94-a75c-4d94-db4c-07fc9a766a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract UBIPr dataset\n",
        "!mkdir -p \"/content/data/UBIPr\"\n",
        "!tar -xvf \"/gdrive/MyDrive/Deep Learning F23 Final Project/Datasets/UBIPr/single_eye/Images.tar\" -C \"/content/data/UBIPr/\"\n",
        "!tar -xvf \"/gdrive/MyDrive/Deep Learning F23 Final Project/Datasets/UBIPr/single_eye/Masks.tar\" -C \"/content/data/UBIPr/\""
      ],
      "metadata": {
        "id": "axCow5ujMr-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a UBIPr Dataset Class\n",
        "class UBIPRDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"Masks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"Masks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:2] # Select only the ID for irises\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "KZ0f3GdawRFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The following code blocks are adapated from Pytorch's Object Detection finetuning tutorial:\n",
        "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
        "\"\"\"\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec3ijceHQhbN",
        "outputId": "2fcba000-02df-40d8-eaeb-81dde1ee3ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_instance_segmentation(num_classes):\n",
        "  \"\"\"\n",
        "  Outputs a MaskRCNN model with replaced classification for finetuning\n",
        "  \"\"\"\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "O_GN4bK8PecV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train):\n",
        "  \"\"\"\n",
        "  Outputs a set of transforms for data augmentation\n",
        "  \"\"\"\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "        transforms.append(T.RandomPhotometricDistort(p=1))\n",
        "        transforms.append(T.RandomIoUCrop())\n",
        "        transforms.append(T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "b4TRXnScQAje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "filename = \"/content/data/UBIPr\"\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and iris\n",
        "num_classes = 2\n",
        "# use our dataset and defined transformations\n",
        "dataset = UBIPRDataset(filename, get_transform(train=True))\n",
        "dataset_test = UBIPRDataset(filename, get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:8800]) # We need to change this to increase the training set size\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[8800:11018]) # We need to change this to increase the test set size\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it for 5 epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "    model_filename = f\"maskrcnn_epoch{epoch}.pt\"\n",
        "    torch.save(model.state_dict(), model_filename)\n",
        "\n",
        "print(\"That's it!\")\n",
        "\n",
        "# End of training loop"
      ],
      "metadata": {
        "id": "Q-63ejRpQMpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Below is code we used to processed the FFHQ and GAN datasets to be used with our Residual Attention Network.\n",
        "We use dlib to extract eye regions from input images, then our fine-tuned MaskRCNN to detect the iris bounding boxes.\n",
        "The irises are then cropped, concatenated into pairs for each input and resized to 96 x 96.\n",
        "\"\"\"\n",
        "\n",
        "# MAKE SURE TO ADD THE PREDICTOR PATH (THE DLIB PREDICTION)\n",
        "# shape_predictor_68_face_landmarks_GTX.dat.bz2\n",
        "def resize_image(image, target_size=(960, 960)):\n",
        "    return cv2.resize(image, target_size)\n",
        "\n",
        "\n",
        "def extract_eye(image, eye_bounds):\n",
        "    return image[eye_bounds[1]:eye_bounds[1] + eye_bounds[3], eye_bounds[0]:eye_bounds[0] + eye_bounds[2]]\n",
        "\n",
        "\n",
        "def crop_eyes(image_folder, predictor_path, output_folder):\n",
        "    # Initialize dlib's face detector and load the facial landmark predictor\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    for i in range(1, 5001):\n",
        "        image_path = os.path.join(image_folder, f\"{str(i).zfill(4)}.jpeg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            continue  # Skip if the file does not exist\n",
        "\n",
        "      # Load the image using OpenCV\n",
        "        image = cv2.imread(image_path)\n",
        "        #image = resize_image(image, (96,96))\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces in the image\n",
        "        faces = detector(gray)\n",
        "\n",
        "        for j, face in enumerate(faces):\n",
        "            landmarks = predictor(gray, face)\n",
        "            # Coordinates for the left and right eye\n",
        "            # https://ibug.doc.ic.ac.uk/resources/300-W/\n",
        "            # https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/\n",
        "            left_eye = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)])\n",
        "            right_eye = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)])\n",
        "\n",
        "            # Cropping the eyes, can also modify the degree of crop so we dont overcrop eyes.\n",
        "            left_eye_bounds = cv2.boundingRect(left_eye)\n",
        "            right_eye_bounds = cv2.boundingRect(right_eye)\n",
        "            margin = 40\n",
        "            left_eye_bounds = (max(left_eye_bounds[0] - margin , 0),max(left_eye_bounds[1] - margin, 0),left_eye_bounds[2] + 2 * margin, left_eye_bounds[3] + 2 * margin)\n",
        "            right_eye_bounds = (max(right_eye_bounds[0] - margin , 0),max(right_eye_bounds[1] - margin, 0),right_eye_bounds[2] + 2 * margin, right_eye_bounds[3] + 2 * margin)\n",
        "\n",
        "            left_eye_image = image[left_eye_bounds[1]:left_eye_bounds[1] + left_eye_bounds[3], left_eye_bounds[0]:left_eye_bounds[0] + left_eye_bounds[2]]\n",
        "            right_eye_image = image[right_eye_bounds[1]:right_eye_bounds[1] + right_eye_bounds[3], right_eye_bounds[0]:right_eye_bounds[0] + right_eye_bounds[2]]\n",
        "\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"left_eye_{i}.jpeg\"), left_eye_image)\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"right_eye_{i}.jpeg\"), right_eye_image)"
      ],
      "metadata": {
        "id": "d0is6IUgHSIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract eye regions and save on local VM,\n",
        "output_directory = \"/content/data/Cropped\"\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "crop_eyes(\"/gdrive/MyDrive/Deep Learning F23 Final Project/Datasets/GAN/GAN/\", \"/gdrive/MyDrive/Deep Learning F23 Final Project/shape_predictor_68_face_landmarks_GTX.dat\", \"/content/data/Cropped\")"
      ],
      "metadata": {
        "id": "8RRqdKXR_Od4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_and_crop_iris(image_path, model, eval_transform):\n",
        "    # Load the image\n",
        "    try:\n",
        "      image = read_image(image_path).float()  # Convert image to floating point tensor\n",
        "    except RuntimeError:\n",
        "      return None\n",
        "\n",
        "    image /= 255.0  # Normalize to [0, 1]\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Perform iris detection using the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = eval_transform(image)\n",
        "        x = x[:3, ...]  # Use only RGB channels\n",
        "        x = x.unsqueeze(0).to(device)  # Add batch dimension\n",
        "        predictions = model(x)\n",
        "        pred = predictions[0]\n",
        "\n",
        "    # Check if iris was detected\n",
        "    if len(pred[\"labels\"]) > 0 and pred[\"scores\"][0] > 0.5:\n",
        "        # Get the coordinates of the bounding box around the iris\n",
        "        pred_boxes = pred[\"boxes\"][:1].long()\n",
        "        x_min, y_min, x_max, y_max = pred_boxes[0]\n",
        "\n",
        "        # Crop the iris\n",
        "        cropped_image = image[:, y_min:y_max, x_min:x_max]\n",
        "\n",
        "        # Convert the cropped tensor to a PIL image\n",
        "        cropped_image_pil = to_pil_image(cropped_image.cpu())\n",
        "\n",
        "        # Resize the cropped iris to 96x96 pixels\n",
        "        resized_iris_pil = cropped_image_pil.resize((48, 96), Image.LANCZOS)\n",
        "        resized_iris_tensor = to_tensor(resized_iris_pil)\n",
        "\n",
        "        # Convert to numpy array and change channel order from RGB to BGR\n",
        "        resized_iris_numpy = resized_iris_tensor.numpy()\n",
        "        resized_iris_numpy = np.transpose(resized_iris_numpy, (1, 2, 0))  # Change order to HWC\n",
        "        resized_iris_numpy = cv2.cvtColor((resized_iris_numpy * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        return resized_iris_numpy\n",
        "    else:\n",
        "        # Iris not detected, return None\n",
        "        return None"
      ],
      "metadata": {
        "id": "eRli9HiSVBD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract eye regions from previous session for iris detection/segmentation\n",
        "!tar -xvf '/gdrive/MyDrive/Deep Learning F23 Final Project/Datasets/Pre-MaskRCNN.tar' -C '/content'"
      ],
      "metadata": {
        "id": "GI4N3mG8RD3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our MaskRCNN from a previous session\n",
        "model_checkpoint = '/gdrive/MyDrive/Deep Learning F23 Final Project/Model Checkpoints/Mask R-CNN/maskrcnn_113023_1210am.pt'\n",
        "num_classes = 2\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "model.load_state_dict(torch.load(model_checkpoint))\n",
        "model.to(device)\n",
        "eval_transform = get_transform(train=False)\n",
        "\n",
        "input_folder = \"/content/Pre-MaskRCNN\"\n",
        "cropped_folder = \"/content/dataset\"\n",
        "iris_not_detected = []\n",
        "iris_size = (96,96)\n",
        "\n",
        "# Detect iris pairs, resize and save\n",
        "for i in range(1,10001):\n",
        "    right_eye_path = os.path.join(input_folder, f\"right_eye_{i}.jpeg\")\n",
        "    left_eye_path = os.path.join(input_folder, f\"left_eye_{i}.jpeg\")\n",
        "\n",
        "    # paste irises next to each other\n",
        "    resized_right_iris = detect_and_crop_iris(right_eye_path, model, eval_transform)\n",
        "    resized_left_iris = detect_and_crop_iris(left_eye_path, model, eval_transform)\n",
        "    if resized_right_iris is not None and resized_left_iris is not None:\n",
        "      combined_iris = np.concatenate((resized_left_iris, resized_right_iris), axis=1)\n",
        "      irises_path = os.path.join(cropped_folder, f\"{i}\".zfill(4) + '.jpg')\n",
        "      cv2.imwrite(irises_path, combined_iris)\n",
        "    elif resized_right_iris is None:\n",
        "      iris_not_detected.append(right_eye_path)\n",
        "    else:\n",
        "      iris_not_detected.append(left_eye_path)\n",
        "\n",
        "# Print the list of images where the iris was not detected\n",
        "print(\"Images with iris not detected:\")\n",
        "for image_path in iris_not_detected:\n",
        "    print(image_path)\n"
      ],
      "metadata": {
        "id": "GjH7BqC1diWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip final dataset\n",
        "!zip -r '/gdrive/MyDrive/Deep Learning F23 Final Project/Datasets/dataset.zip' '/content/dataset'"
      ],
      "metadata": {
        "id": "DE6bZv-PnpSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}